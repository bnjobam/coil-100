from sklearn.decomposition import PCA
import xgboost as xgb
from sklearn.pipeline import make_pipeline
import pandas as pd
import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
import os
import shutil
from PIL import Image


def graycells(image):
    n,m =image.shape[0],image.shape[1]
    im=np.zeros((n,m))
    for i in range(n):
        for j in range(m):
            im[i][j]=image[i][j].mean()
    return im

def read_data(path):
    data=[]
    id_cat=[]
    angle=[]
    for imagePath in os.listdir(path):
        # load the image, pre-process it, and store it in the data list
        image = Image.open(path+'/'+imagePath)
        image = image.resize((IMAGE_DIMS[1], IMAGE_DIMS[0]))
        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = np.asarray(image)
        data.append(image)

        # update the respective lists
        id_cat.append(imagePath.split('__')[0])
        angle.append(float(imagePath.split('__')[1].split('.')[0]))
    return data, id_cat, angle

folder='C:/Users/My PC/Documents/GitHub/coil100'

train_dir = f'{folder}/train'
validation_dir = f'{folder}/valid'

trainY=os.listdir(train_dir)
validY=os.listdir(validation_dir)

# initialize the number of epochs to train for, initial learning rate,
# batch size, and image dimensions

IMAGE_DIMS = (96, 96, 3)

#Obtain the data sets for training, validation and testing
train_data, trainY, tangle=read_data(train_dir)
valid_data, validY, vangle=read_data(validation_dir)
train_data=np.array(train_data)
valid_data=np.array(valid_data)
tangle=np.array(tangle)
tangle=tangle.astype(np.float)
vangle=np.array(vangle)
vangle=vangle.astype(np.float)

train=np.array([graycells(x) for x in train_data])
train1=np.array([train[i].reshape(train[0].shape[0]*train[0].shape[1]) for i in range(len(train))])

valid=np.array([graycells(x) for x in valid_data])
valid1=np.array([valid[i].reshape(valid[0].shape[0]*valid[0].shape[1]) for i in range(len(valid))])


pca = PCA(n_components=600, whiten=False, random_state=42)
xgbregress=xgb.XGBRegressor(random_state=1, learning_rate=0.2, n_jobs=-1)

model=make_pipeline(pca,model)

track=[]
for k in range(1000,6000,1000):
    predmodel=model.fit(train1[:k+1],tangle[:k+1]/355)
    pred=predmodel.predict(valid1)*355
    track.append(sum([(pred[i]-np.asarray(vangle)[i])**2/len(vangle) for i in range(len(vangle))]))

plt.figure()
plt.plot([1000,2000,3000,4000,5000],track)
plt.ylabel('MSE')
plt.xlabel('Number of samples')
plt.title('Training size against MSE')
plt.savefig(folder+'/plots and displays/Train data size VS MSE.png')
plt.show()
